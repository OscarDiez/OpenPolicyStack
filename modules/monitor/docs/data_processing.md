# Data Processing


The data processing file contains classes that process the data. Processing the data means adding new columns to the raw data with content that is generated based on the existing columns. This functionality can be used for filterig or categorizing. 

Currently there are two data processing steps implemented:
- a keyword-based scoring functionality in the class ```KeywordMatchScorer```
- a LLM-based catergorising functionality in the class ```LLMCategorizer```


## Keyword Scorer

The keyword scorer (more precisely, the method ```compute_add_match_score```) assigns a match score to each project based on the provided list of keywords. A high matchscore indicates that many keywords appear in the description and the title of the project. Thus, the project is considered a good match. The project is computed by going through each keyword and adding ```+= (1-(pos/float(len(sentence))))``` to the match score of the project, where ```pos``` is the position of the appearence(s) of the keyword and sentence is the description of the project. So the weight of a keyboard appearence is stronger when it occurs at the beginning of the project description.

The match score and the keywords found in the description are added as additional columns to the inout data. 

The method ```get_filtered_data``` class also implements a filter based on the match score and returns only the projects (and the corresponding organizations) with a matchscore exceeding a certain threshold ```threshold```. For assisting the user in choosing a good threshold, the method ```plot_matchscore_histogram``` plots a match score histogram, revealing how many projects received which match score. This histogram may look like this: 

![matschore_hostogram](figures/matchscore_histogram.png)

Ideally one finds a bimodal distribution and adjusts the threshold such that it seperates the two peaks. If this is not the case, one may have to look at the raw data and check manually which threshold would optimally seperate relevant from irrelevant projects. When also using the ```LLMCategorizer``` class (see below) for filtering, one may choose the keywords and/or thre threshold in a way that only the obviously wrong projects are eliminated, while the better filtering is left to the LLM. 


## LLM Categorizer

The ```LLMCategorizer``` class uses LLMs to categorize each project. In particular, this is implemented by the ```categorize``` method. There are two different options for the LLM host, i.e. the location of where the LLM is hosted:
- *Local*: When the parameter ```model_location``` of functiom ```categorize``` is set to "local", EFMO attempts to use a local Ollama server. This requires OllamaLLM and the corresponding python wrapper to be installed (instructions can be found [here](https://github.com/ollama/ollama-python)). This option requires a *very* powerful hardware setup, because running LLMs locally is computationally very expensive. On the EC2 instance, mentioned in the [hardware description](hardware.md), each request to the LLM ```Meta-Llama-3.3-70B-Instruct``` takes more than five minutes. Since there is one request per project, applying the categorization to all projects after the keyword filter (typically a few thousand) would take many ideas which is impractical. 
- *Remote (SambaNova)*: When the parameter ```model_location``` of functiom ```categorize``` is set to "remote", EFMO attempts to use the LLMs provided by [SambaNova](https://cloud.sambanova.ai/) through their API. The default model which turns to be a good compromise between speed and capabilities is ```Meta-Llama-3.3-70B-Instruct``` (hardcoded in the class). The API key belongs to Julian Wienand's private SambaNova account. At the time of coding, the [API's rate limit](https://docs.sambanova.ai/cloud/api-reference/using-the-api/rate-limits) was 300 per hour and 3600 per day. Since one project is processed at a time (one may change that), the LLM categorization is currently limited to 3600 projects. Obviously this is a temporary solution that only works for the prototype of EFMO. The current API should be replaced with that of GPT@EC as soon as possible. As an alternative, not all projects may have to be categorized every week, but only the ones that have been added to the data. However, this only works as long as the categorization is not changed. In order to deal with the rate limit, a request for a certain project is repeated up to 20 times until it succeeds with a 5min break between the attempts. 


The prompt sent to the LLM for each project is generated by the ```get_prompt``` method. This method takes the description of the project and attaches it to the categorization prompt. 


The output of the LLM is added to the input data as a new column called "LLMCategory". The output may contain multiple pieces of information about the project, e.g. separated by comma. In that case the column needs to be further processed, e.g. in a later part of the workflow. 



